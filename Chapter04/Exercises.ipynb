{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.8 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Prove the following is equivalent:\n",
    "\n",
    "$$\\begin{align}\n",
    "    p\\left(X\\right) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\tag{4.2}\\\\\n",
    "    &\\Updownarrow\\nonumber\\\\\n",
    "    \\frac{p\\left(X\\right)}{1 - p\\left(X\\right)} &= e^{\\beta_0 + \\beta_1 X}\\tag{4.3}\n",
    "\\end{align}$$\n",
    "\n",
    "### Answer\n",
    "\n",
    "$$\\begin{align*}\n",
    "    p\\left(X\\right) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\tag{4.2}\\\\\n",
    "    &\\Updownarrow\\nonumber\\\\\n",
    "    p\\left(X\\right)\\left(1 + e^{\\beta_0 + \\beta_1 X}\\right) &= e^{\\beta_0 + \\beta_1 X}\\\\\n",
    "    &\\Updownarrow\\nonumber\\\\\n",
    "    p\\left(X\\right) &= e^{\\beta_0 + \\beta_1 X}\\left(1 - p\\left(X\\right)\\right)\\\\\n",
    "    &\\Updownarrow\\nonumber\\\\\n",
    "    \\frac{p\\left(X\\right)}{1 - p\\left(X\\right)} &= e^{\\beta_0 + \\beta_1 X}\\tag{4.3}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk, σ2) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n",
    "\n",
    "$$\\tag{4.17}\n",
    "    p_k\\left(x\\right) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_l\\right)^2\\right)}\n",
    "$$\n",
    "\n",
    "$$\\tag{4.18}\n",
    "    \\delta_k\\left(x\\right) = x\\cdot\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log\\left(\\pi_k\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "- We assume that $x$ has distribution $N\\left(\\mu_k, \\sigma^2\\right)$ that it comes from $k$-th class\n",
    "- We assume $x$ is observed\n",
    "- We will start by taking log of $\\left(4.17\\right)$\n",
    "- We note that the denominator of the fraction in $\\left(4.17\\right)$ is independent of $k$ when we are maximizing over $k$ as that sum will be the same for any $k$\n",
    "- Therefore, we will only focus on the numerator:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\log\\left(\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\right)\\right)\n",
    "    &= \\log\\left(\\pi_k\\right) -\\underbrace{\\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right)}_{\\text{independent of } k} -\\frac{1}{2\\sigma^2}\\left(x-\\mu_k\\right)^2\\\\\n",
    "    &= \\log\\left(\\pi_k\\right) -\\underbrace{\\frac{x^2}{2\\sigma^2}}_{\\text{independent of } k} + x\\cdot\\frac{\\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2\\sigma^2}\\\\\n",
    "    &= x\\cdot\\frac{\\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2\\sigma^2} + \\log\\left(\\pi_k\\right)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "- As log is a strictly increasing function, maximizing $\\left(4.17\\right)$ is the same as maximizing $\\left(4.18\\right)$ with respect to $k$ for a fixed (observed) $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, X ∼ N(µk, σ2k). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n",
    "\n",
    "Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that σ21 = ··· = σ2K.\n",
    "\n",
    "$$\\tag{4.16}\n",
    "f_k\\left(x\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}\\left(x-\\mu_k\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "### Answer\n",
    "\n",
    "- We assume that $x$ has distribution $N\\left(\\mu_k, \\sigma_k^2\\right)$ that it comes from $k$-th class\n",
    "- We assume $x$ is observed\n",
    "- We will start by taking log of the following equation:\n",
    "$$\n",
    "    p_k\\left(x\\right) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}\\left(x-\\mu_k\\right)^2\\right)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma_l^2}}\\exp\\left(-\\frac{1}{2\\sigma_l^2}\\left(x-\\mu_l\\right)^2\\right)}\n",
    "$$\n",
    "- We note that the denominator of the fraction in the above equation is independent of $k$ when we are maximizing over $k$ as that sum will be the same for any $k$\n",
    "- Therefore, we will only focus on the numerator:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\log\\left(\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}\\left(x-\\mu_k\\right)^2\\right)\\right)\n",
    "    &= \\log\\left(\\pi_k\\right) -\\frac{1}{2}\\log\\left(2\\pi\\sigma_k^2\\right) - \\frac{1}{2\\sigma_k^2}\\left(x-\\mu_k\\right)^2\\\\\n",
    "    &= \\log\\left(\\pi_k\\right) -\\frac{1}{2}\\log\\left(2\\pi\\sigma_k^2\\right) -\\frac{x^2}{2\\sigma_k^2} + x\\cdot\\frac{\\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2\\sigma^2}\\\\\n",
    "    &= -x^2\\cdot\\frac{1}{2\\sigma_k^2} + x\\cdot\\frac{\\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2\\sigma^2} + \\log\\left(\\pi_k\\right)-\\frac{1}{2}\\log\\left(2\\pi\\sigma_k^2\\right)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "- We can see that the final line is a quadratic function of $x$ and not linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. \n",
    "\n",
    "When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.\n",
    "\n",
    "a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?\n",
    "\n",
    "### Answer \n",
    "\n",
    "- $10\\%$\n",
    "\n",
    "b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?\n",
    "\n",
    "### Answer\n",
    "\n",
    "- $1\\% = 10\\%^2$\n",
    "\n",
    "c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n",
    "\n",
    "### Answer\n",
    "\n",
    "- $10\\%^{100} = \\left(10^{-1}\\right)^{-100} = 10^{-100}$\n",
    "\n",
    "d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\n",
    "\n",
    "### Answer\n",
    "- We can see that the number of training observations near any given test observation decreases exponentially\n",
    "- Therefore, we will start to overfit very quickly\n",
    "- In other words if we want to avoid overfitting, we would need to have exponentially more data points for each new dimension of our feature\n",
    "\n",
    "e) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer\n",
    "\n",
    "Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.\n",
    "\n",
    "### Answer\n",
    "\n",
    "|$p$|length of hypercube|\n",
    "|---|---|\n",
    "| 1 |0.1|\n",
    "| 2 |0.3|\n",
    "|100|0.98|\n",
    "\n",
    "- We get the length of hypercube $l$ by the following formula (where $p$ is the number of features):\n",
    "\n",
    "$$ l^p = 0.1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

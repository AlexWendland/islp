{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d037b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# cd .\\Chapter07\n",
    "# jupyter nbconvert .\\Main.ipynb --to slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106ab77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 7 - Moving Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d006b-acd6-408a-b1ee-cd9f7b65d68a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relaxing the assumption of linearity\n",
    "\n",
    "Recall some notation, we have $p$ predictors $X=\\left(X_{1}, X_{2}, \\cdots X_{p}\\right)$ and a response $Y$ and we assume that there is some relationship between $X$ and $Y$ that we can represent as \n",
    "\n",
    "$$Y = f\\left(X\\right) + \\epsilon$$\n",
    "\n",
    "Additionally, recall that we do not know the true $f$ and therefore are trying to estimate it. Our estimate is denoted by $\\hat{f}$. The value for the response produced by this estimated function is $\\hat{Y}$. So far we have restricted the form of $\\hat{f}$ to be linear. \n",
    "\n",
    "$$\\hat{Y} = X\\beta $$\n",
    "\n",
    "We now relax this assumption and build on the linear models to maintain interpretibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30921047-729d-49f3-aa2a-35ac8a5c7d6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different methods\n",
    "\n",
    "The following methods build on the linear regression methods \n",
    "\n",
    "1. Polynomial regression - adding extra predictors of the form $X^{r}$ (tend to use  $r < 4$ otherwise unusual/unlikely shapes)\n",
    "1. Step functions - fitting a piecewise constant function, for $K$ distinct regions. Effectively creating a qualitative variable.\n",
    "1. Regression Splines - Extension of 1,2. Split into $K$ regions and fit a polynomial. Constrained to join smoothly at the boundaries\n",
    "1. Smoothing Splines - Similar to regression spline, but motivated differently. Arise from minimizing residual sum of squares subject to smoothness penalty\n",
    "1. Local regression - Local Similar to splines but regions are allowed to overlap. \n",
    "1. Generalized Additive Models - generalizes the above methods to deal with multiple predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77da5a-d16d-498a-a75b-697e1d041ebf",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "$$ \\hat{f}\\left(x_0\\right) = \\sum_{j=0}^d \\beta_jx_0^d $$\n",
    "\n",
    "- Rule of thumb - $d < 4$ to avoid overfitting and strange shapes especially at the boundaries of $x$\n",
    "- We will only be working with one feature and taking the polynomial expansion of that\n",
    "- We can fit this using normal linear regression with features $X_1 = X, X_2 = X^2, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872d976",
   "metadata": {},
   "source": [
    "### Example Standard Error\n",
    "\n",
    "![](images/polynomial_standard_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6eaf6",
   "metadata": {},
   "source": [
    "### Equations\n",
    "\n",
    "Suppose $y$ is wage and $x$ is age\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + \\beta_4 x_i^4 + \\epsilon_i\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}\\left[y_i > 250 \\middle| x_i\\right] = \\frac{\\exp\\left(\\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + \\beta_4 x_i^4\\right)}{1 + \\exp\\left(\\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + \\beta_4 x_i^4\\right)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc00ac",
   "metadata": {},
   "source": [
    "### Side Track\n",
    "\n",
    "- We can estimate the standard error of $\\hat{f}\\left(x_0\\right)$ to be the squareroot of the variance of $\\hat{f}\\left(x_0\\right)$\n",
    "- Where $\\mathbb{V}\\left[\\hat{f}\\left(x_0\\right)\\right] = l_0'\\hat{\\mathbf{C}}l_0$ where $l_0' = \\left(1,x_0,x_0^2,...,x_0^d\\right)$ and $\\hat{\\mathbf{C}} = \\mathbb{V}\\left[\\hat{\\beta}\\right]$\n",
    "- The standard error explodes because the size of $x_0^d$ will increase exponentially as $x_0$ increases\n",
    "- TODO: add picture from book\n",
    "- TODO: derive $\\mathbb{V}\\left[\\hat{\\beta}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354cb94e-6e58-4ce0-af11-e0e4c10266d1",
   "metadata": {},
   "source": [
    "## Step Functions\n",
    "\n",
    "- We partition our feature space and fit a constant for each partition\n",
    "- Doing this will result in the fitted function to be a step function\n",
    "- Let us start with the standard table:\n",
    "\n",
    "| $y$ | $X$ |\n",
    "| -- | -- |\n",
    "| $y_1$ | $x_1$ |\n",
    "| $y_2$ | $x_2$ |\n",
    "| $y_3$ | $x_3$ |\n",
    "| $y_4$ | $x_4$ |\n",
    "| $y_5$ | $x_5$ |\n",
    "| $y_6$ | $x_6$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc310",
   "metadata": {},
   "source": [
    "\n",
    "- Suppose we create the partition with $x_1, x_2$, $x_3, x_4$ and $x_5, x_6$, ignoring the constant term we have:\n",
    "\n",
    "| $y$ | $\\tilde{X}_1$ | $\\tilde{X}_2$ | $\\tilde{X}_3$ |\n",
    "| -- | -- | -- | -- |\n",
    "| $y_1$ | 1 | 0 | 0 |\n",
    "| $y_2$ | 1 | 0 | 0 |\n",
    "| $y_3$ | 0 | 1 | 0 |\n",
    "| $y_4$ | 0 | 1 | 0 |\n",
    "| $y_5$ | 0 | 0 | 1 |\n",
    "| $y_6$ | 0 | 0 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad7930",
   "metadata": {},
   "source": [
    "### Example Standard Error\n",
    "\n",
    "![](images/step_standard_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130da846",
   "metadata": {},
   "source": [
    "### Equations\n",
    "\n",
    "Suppose $y$ is wage and $x$ is age\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 \\tilde{x}_1 + \\beta_2 \\tilde{x}_2 + \\beta_3 \\tilde{x}_3 + \\epsilon_i\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}\\left[y_i > 250 \\middle| x_i\\right] = \\frac{\\exp\\left(\\beta_0 + \\beta_1 \\tilde{x}_1 + \\beta_2 \\tilde{x}_2 + \\beta_3 \\tilde{x}_3\\right)}{1 + \\exp\\left(\\beta_0 + \\beta_1 \\tilde{x}_1 + \\beta_2 \\tilde{x}_2 + \\beta_3 \\tilde{x}_3\\right)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99fbbd",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "- If there are no natural break points, you can miss key changes in the data\n",
    "- Choosing the break points can be problematic\n",
    "- We call the break points cut points or knots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d090a09",
   "metadata": {},
   "source": [
    "## Piecewise Polynomials\n",
    "\n",
    "- This is combining the two previous concepts where we will use a polynomial expansion on our feature but then fit separate models for each partition\n",
    "- Example:\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "\\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i + \\beta_{31}x_i + \\epsilon_i & \\text{if } x_i < c\\\\\n",
    "\\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i + \\beta_{32}x_i + \\epsilon_i & \\text{if } x_i \\geq c\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb50deb",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "- Polynomial and step functions are special cases of basis function approach, all of these benefit from using the tools of regression\n",
    "- Suppose we have functions $b_1\\left(X\\right), \\cdots, b_k\\left(X\\right)$ which are fixed and known ahead of time which form the new set of features\n",
    "- Examples are wavelets and Fourier series as well regression splines (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343f06a-1123-4dba-af73-5d054a8fc195",
   "metadata": {},
   "source": [
    "## Regression Splines\n",
    "\n",
    "- Degree $D$ spline is a piecewise polynomial with degree $D$ with continuity in derivatives up to degree $D-1$\n",
    "- We can represent the cubic spline using truncated power basis functions defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "    f\\left(x\\right) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{k=1}^K\\beta_{k+3} \\left(x-\\xi_k\\right)^3_+\n",
    "\\end{align}$$\n",
    "\n",
    "- Where $\\xi_k$ denotes the knots for $k=1,\\dots, K$ and\n",
    "\n",
    "$$\n",
    "\\left(x-\\xi_k\\right)^3_+ = \n",
    "\\begin{cases}\n",
    "\\left(x-\\xi_k\\right)^3 & \\text{if } x>\\xi_k\\\\\n",
    "0 & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Exercise 1 in conceptual exercises shows that with one knot, the truncated power basis functions represent cubic spline\n",
    "- With the truncated power basis function representation, it is easy to see how to fit linear regression with it\n",
    "- There are other ways to represent a cubic spline (TODO: give an example)\n",
    "- This generalizes to order $D$ splines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dfaef",
   "metadata": {},
   "source": [
    "### Degrees of Freedom\n",
    "\n",
    "- For a general linear regression the number of degrees of freedom is $p$ where $p$ is number of features (including the intercept)\n",
    "- For a cubic spline, we have the features $x^0, x^1, x^2, x^3$ and $\\left(x-\\xi_k\\right)^3$ where $k=1,\\dots,K$ hence the number of degrees of freedom is $K+4$\n",
    "- For piecewise polynomial regression with $K$ knots, we have to fit $K+1$ different models with degree $D$ polynomials, giving us $\\left(D + 1\\right)\\times\\left(K+1\\right)$ features and therefore number of degrees of freedom. Too flexible!\n",
    "- Each constraint we add gives one less degree of freedom, therefore for the cubic case, we add three constraints - continuity, first derivative continuity and second derivative continuity\n",
    "- For one knot, top left chart we start with $8 = 4 \\times 2$ degrees of freedom for piecewise cubic, and by adding the constraints for the spline, we end up with $ 8 - 3 = 5$ degrees of freedom bottom left. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456ea67",
   "metadata": {},
   "source": [
    "### Piecewise Verses Spline\n",
    "\n",
    "![](images/piecewise_vs_spline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f1b00",
   "metadata": {},
   "source": [
    "### Natural Cubic Spline\n",
    "\n",
    "- Extrapolate linearly beyond the boundary knot, adding 4 (2 edge partitions and 2 fewer degrees of polynomials i.e. no quadratic and cubic) extra constraints\n",
    "\n",
    "![](images/natural_cubic_spline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434aaf9",
   "metadata": {},
   "source": [
    "### Choosing Knots\n",
    "\n",
    "- Place more knots where the functions varies more rapidly (maybe use rolling standard deviation to define this)\n",
    "- Place knots in uniformly, e.g. quantiles of the data\n",
    "- Cross validation for the number of knots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c482f-12cf-42a7-bf6c-225b639a3af2",
   "metadata": {},
   "source": [
    "## Smoothing Splines\n",
    "\n",
    "- We start with the following loss function:\n",
    "$$\\text{RSS} = \\sum_{i=1}^n\\left(y_i - g\\left(x_i\\right)\\right)^2$$\n",
    "- If $g\\left(x\\right)$ is unconstrained then $\\text{RSS}$ can be made to be zero but we overfit. The function in this case is unlikely to be smooth\n",
    "- Therefore, we could minimize $\\text{RSS}$ subject to the constraint that $g$ is smooth\n",
    "- The loss function for smoothing splines is given by:\n",
    "\n",
    "$$\\sum_{i=1}^n\\left(y_i - g\\left(x_i\\right)\\right)^2 + \\lambda \\int g''\\left(t\\right)^2dt$$\n",
    "\n",
    "- $\\lambda$ is the smoothing penalty, as $\\lambda$ increases, $g$ will be smoother, it controls the bias variance tradeoff\n",
    "- $g''$ is approximately the roughness of the function, $g''$ is zero for a straight line, i.e. as smooth as it gets\n",
    "- $\\int g''\\left(t\\right)^2dt$ is measuring the total change in $g'$ over the range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44daf8",
   "metadata": {},
   "source": [
    "### Smoothing Spline Solution\n",
    "\n",
    "- $g\\left(x\\right)$ which minimizes the smooth penalty loss function has the following properties:\n",
    "  1. It is a piecewise cubic polynomial with knots at unique values of $x_1,\\dots,x_n$\n",
    "  1. It is continuous at the first and second derivatives at the knots\n",
    "  1. It is linear outside of the boundary knots\n",
    "- Therefore, it is a natural cubic with knots at $x_1,\\dots,x_n$\n",
    "- It is not the same natural cubic spline that comes from the basis function approach, it is a shrunken version where $\\lambda$ controls the level of shrinkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1ca54",
   "metadata": {},
   "source": [
    "### Choosing $\\lambda$\n",
    "\n",
    "- $\\lambda$ controls the smoothness and hence the degrees of freedom\n",
    "- As $\\lambda$ increases from zero to infinity, the effective degrees, $df_\\lambda$, of freedom decrease from $n$ to 2\n",
    "- Use cross validation to choose $\\lambda$, can use leave one out which efficient for smoothing splines\n",
    "- You could choose degrees of freedom rather than $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e08c0e-c2fb-434c-a296-9f563b1085e9",
   "metadata": {},
   "source": [
    "## Local Regression\n",
    "\n",
    "- $s$ is denoted as span which is the proportion of points used to compute the local regression. It controls the flexibility of the regression\n",
    "- Algorithm for $X=x_0$\n",
    "  1. Take $s=k/n$ data points closest to $x_0$\n",
    "  1. Assign a weight $K_{i,0} = K\\left(x_i,x_0\\right)$ to each point in the neighborhood where the furthest point 0 and the closest has the highest weight\n",
    "  1. Fit a weighted least squares regression of $y_i$ on $x_i$ finding $\\beta$ that minimizes:\n",
    "    $$\\begin{align}\\sum_{i=1}^n K_{i,0}\\left(y_i - \\beta_0 - \\beta_1x_i\\right)^2\\end{align}$$\n",
    "  4. The fitted value at $x_0$ is given by $\\hat{f}\\left(x_0\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$\n",
    "- Variants:\n",
    "  - Different weighting functions\n",
    "  - You could do a polynomial expansion on the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39512d",
   "metadata": {},
   "source": [
    "![](images/local_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618ae1d-654c-444d-bdd7-16b207facb75",
   "metadata": {},
   "source": [
    "## Generalized Additive Models\n",
    "\n",
    "- Extend multiple linear regression by allowing non linear functions\n",
    "- We replace the standard linear equation with smooth non linear functions in the following way:\n",
    "\n",
    "$$\\begin{align}y_i = \\beta_0 + \\sum_{j=1}^p f_j\\left(x_{ij}\\right) + \\epsilon_i\\end{align}$$\n",
    "\n",
    "- It is called an additive model we calculate a separate $f_j$ for each $X_j$ and then add together all their contributions\n",
    "- We can easily do a spline via this method but not smoothing splines\n",
    "- Use back fitting to fit GAMs using smoothing splines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e6810-92f2-4ac6-a86e-6e197c592a38",
   "metadata": {},
   "source": [
    "## Summary of Methods\n",
    "\n",
    "| Method | Advantages | Disadvantages | Notes |\n",
    "| -- | -- | -- | -- |\n",
    "| Polynomial regression | Simple way to provide non linearity | Notorious tail behaviour, bad for extrapolation | Limit to degree less than 4 |\n",
    "| Step function | Simple and popular in biostatistics and epidemiology | Miss relationships at the break points, choosing the breakpoints can be difficult |  |\n",
    "| Piecewise polynomial | | Discontinuities at break points. High degrees of freedom | |\n",
    "| Spline | No need to go beyond degree 3 as the discontinuity is not visible to human eye. Introduce flexibility without adding too many degrees of freedom (leads to more stable estimates compared to piecewise polynomial) | High variance on outer range (improved by natural spline) | |\n",
    "| Smoothing Spline | It is a natural cubic spline. Do not need to specify knots. Can specify $\\lambda$ through degrees of freedom. | | |\n",
    "| Local regression | Allows to fit a model for global in some variables and local in others | Need all training data for prediction. Performs poorly for $p>3$ due to curse of dimensions | |\n",
    "| Generalized additive model | We can see the contribution of each $X_j$ onto $y$. More accurate predictions compared to linear. | Main limitation is that it has to be additive | |\n",
    "\n",
    "Placeholder to put a table of advantages and disadvantages of the different methods\n",
    "\n",
    "1. Polynomial regression - simple way to provide non-linear fit\n",
    "1. Step functions - fitting a piecewise constant function\n",
    "1. Regression Splines\n",
    "1. Smoothing Splines\n",
    "1. Local regression\n",
    "1. Generalized Additive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d5c1a-6034-41e3-8c3f-0e156e2c2f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
